[runtime]
; emcee fisher test importance multinest
sampler = importance
;root = ${COSMOSIS_SRC_DIR}

[importance]
; The output of a previous cosmosis run that you want to combine likelihoods with 
input = /data/des41.a/data/marcelle/gw/cosmosis/gw/chains/2pt_NG.fits_d_w_chain_trimmed.txt
; The number of likelihood computations performed between each line of output 
nstep = 128
; In computing the importance of the input samples, we can add the new
; likelihood to the prior ones (instead of replacing them entirely).
add_to_likelihood = T

[test]
save_dir = test

[fisher]
step_size = 0.02

[emcee]
walkers = 64
samples = 500
nsteps = 100

[multinest]
max_iterations=50000
;multinest_outfile_root=mn_%(RUN_NAME)s
multinest_outfile_root=o2_
resume=F
;- Suggested standard run:
;live_points=500
;efficiency=0.3
;tolerance=0.1
;constant_efficiency=F
;- A mega-run:
live_points=1000
efficiency=0.05
tolerance=0.1
constant_efficiency=T

[output]
filename = o3_sim_test_sigma8.txt
format = text
verbosity= debug

[pipeline]
modules = consistency camb gwem
values = ${DES_GW_WORK_DIR}/values.ini
;extra_output = cosmological_parameters/sigma_8   
likelihoods = gwem
quiet=T
debug=F
timing=F

[consistency]
file = cosmosis-standard-library/utility/consistency/consistency_interface.py

[camb]
file = cosmosis-standard-library/boltzmann/camb/camb.so
mode=background
feedback=0
zmin=0.0
zmax=0.2
nz=100

[gwem]
file = ${DES_GW_WORK_DIR}/gwem.py
z_frame = cmb
;z_frame = helio
data_dir = ${DES_GW_WORK_DIR}/data
;data_file = ${DES_GW_WORK_DIR}/data/gw170817-holz.txt
data_file = ${DES_GW_WORK_DIR}/data/test.txt

